{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from cleanlab import Datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '부채_대비_소득_비율'이 5000보다 큰 행의 인덱스를 찾기\n",
    "index_to_drop = train_df[train_df['부채_대비_소득_비율'] > 5000].index\n",
    "\n",
    "# 찾은 인덱스를 사용하여 해당 행을 삭제\n",
    "train_df.drop(index_to_drop, inplace=True)\n",
    "\n",
    "# 삭제한 행으로 인해 인덱스를 리셋\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피처 엔지니어링1 (피처 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '총상환원금비율', '총상환이자비율' 피처 생성\n",
    "train_df['총상환원금비율'] = (train_df['총상환원금']/train_df['대출금액']) * 100\n",
    "test_df['총상환원금비율'] = (test_df['총상환원금']/test_df['대출금액']) * 100\n",
    "train_df['총상환이자비율'] = (train_df['총상환이자']/train_df['대출금액']) * 100\n",
    "test_df['총상환이자비율'] = (test_df['총상환이자']/test_df['대출금액']) * 100\n",
    "\n",
    "# '연간소득/대출금액' 피처 생성\n",
    "train_df['연간소득/대출금액'] = train_df['연간소득']/train_df['대출금액']\n",
    "test_df['연간소득/대출금액'] = test_df['연간소득']/test_df['대출금액']\n",
    "\n",
    "\n",
    "# '총상환이자', '총상환원금' 피처 drop\n",
    "train_df = train_df.drop(columns=['총상환이자', '총상환원금'])\n",
    "test_df = test_df.drop(columns=['총상환이자', '총상환원금'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **총상환원금비율, 총상환이자비율:** '대출금액'이 많을수록 '총상환원금'과 '총상환이자'는 당연히 수가 클거라 판단 후 '총상환원금', '총상환이자'를 '대출금액'으로 나누어서 비율로 보기로 결정\n",
    "\n",
    "\n",
    "- **연간소득/대출금액:** 대출금액에 따른 연간소득을 살펴보았을 때 본인의 소득 수준에서 대출을 얼마나 하는지 알 수 있을 것이라 판단 후 '연간소득'을 '대출금액'으로 나누어서 새 피처를 생성\n",
    "\n",
    "\n",
    "- **drop:** 피처 생성 후 더이상 필요 없을 것 같은 피처 drop ('총상환이자', '총상환원금')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그변환\n",
    "columns_to_transform = ['연간소득', '부채_대비_소득_비율', '총상환원금비율', '총상환이자비율', '총연체금액']\n",
    "\n",
    "train_df[columns_to_transform] = train_df[columns_to_transform].apply(lambda x: np.log1p(x))\n",
    "test_df[columns_to_transform] = test_df[columns_to_transform].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- skewness가 큰 데이터셋이기 때문에 로그 스케일링 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피처 엔지니어링2 (피처 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '연체계좌수/총계좌수', '연간소득/총계좌수' 피처 생성\n",
    "train_df['연체계좌수/총계좌수'] = (train_df['연체계좌수']/train_df['총계좌수']) * 100\n",
    "test_df['연체계좌수/총계좌수'] = (test_df['연체계좌수']/test_df['총계좌수']) * 100\n",
    "train_df['연간소득/총계좌수'] = (train_df['연간소득']/train_df['총계좌수']) * 100\n",
    "test_df['연간소득/총계좌수'] = (test_df['연간소득']/test_df['총계좌수']) * 100\n",
    "\n",
    "# '연체계좌수', '총계좌수', 피처 drop\n",
    "train_df = train_df.drop(columns=['연체계좌수', '총계좌수'])\n",
    "test_df = test_df.drop(columns=['연체계좌수', '총계좌수'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***연간소득 스케일링 진행 후 피처 생성***\n",
    "\n",
    "\n",
    "- **연체계좌수/총계좌수:** 총계좌 중 얼마나 많은 계좌가 연체되었는지의 정보를 통해 대출등급을 추정할 수 있을 것이라 판단 후 '연체계좌수'를 '총계좌수'로 나누어서 새 피처를 생성\n",
    "\n",
    "\n",
    "- **연간소득/총계좌수:** 소득에 따라 계좌를 얼마나 가지고 있는지의 정보를 통해 대출등급을 추정할 수 있을 것이라 판단 후 (돈은 없고 계좌만 많은 사람 등등) '연간소득'를 '총계좌수'로 나누어서 새 피처를 생성\n",
    "\n",
    "\n",
    "- **drop:** 피처 생성 후 더이상 필요 없을 것 같은 피처 drop ('연체계좌수', '총계좌수')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 범주형 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 근로기간 0~10 범주로 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_work_experience(value):\n",
    "    if value == 'Unknown':\n",
    "        return None\n",
    "    elif '< 1 year' in value or '<1 year' in value:\n",
    "        return '0'\n",
    "    else:\n",
    "        match = re.search(r'\\d+', str(value))\n",
    "        return match.group() if match else None\n",
    "\n",
    "# Apply the cleaning function to '근로기간' column\n",
    "train_df['근로기간'] = train_df['근로기간'].apply(clean_work_experience)\n",
    "test_df['근로기간'] = test_df['근로기간'].apply(clean_work_experience)\n",
    "\n",
    "# Convert the column to numeric type\n",
    "train_df['근로기간'] = pd.to_numeric(train_df['근로기간'], errors='coerce')\n",
    "test_df['근로기간'] = pd.to_numeric(test_df['근로기간'], errors='coerce')\n",
    "\n",
    "# 결측치 처리\n",
    "train_df['근로기간'].fillna(round(train_df['근로기간'].mean()), inplace=True)\n",
    "test_df['근로기간'].fillna(round(train_df['근로기간'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주택소유상태의 ANY를 MORTAGE(최빈값)으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['주택소유상태'] = train_df['주택소유상태'].replace({'ANY':'MORTGAGE'})\n",
    "test_df['주택소유상태'] = test_df['주택소유상태'].replace({'ANY':'MORTGAGE'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대출목적의 범주를 '부채통합',' '주택', '신용카드', '기타'의 4개의 범주로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['대출목적'] = train_df['대출목적'].replace({'이사':'주택', '주택개선':'주택'})\n",
    "train_df.loc[~train_df['대출목적'].isin(['주택', '부채 통합', '신용 카드']), '대출목적'] = '기타'\n",
    "test_df['대출목적'] = test_df['대출목적'].replace({'이사':'주택', '주택개선':'주택'})\n",
    "test_df.loc[~train_df['대출목적'].isin(['주택', '부채 통합', '신용 카드']), '대출목적'] = '기타'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['ID'])\n",
    "test_df = test_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['대출기간', '주택소유상태', '대출목적']\n",
    "\n",
    "for i in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le=le.fit(train_df[i]) \n",
    "    train_df[i]=le.transform(train_df[i])\n",
    "    \n",
    "    for case in np.unique(test_df[i]):\n",
    "        if case not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, case) \n",
    "    test_df[i]=le.transform(test_df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 피처 Label Encoding\n",
    "target_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6} \n",
    "reverse_target_dict = {v: k for k, v in target_dict.items()} #submission을 위해 재변환을 위한 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 피처 Label Encoding\n",
    "target_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6} \n",
    "reverse_target_dict = {v: k for k, v in target_dict.items()} #submission을 위해 재변환을 위한 딕셔너리\n",
    "# apply 함수를 사용하여 Label Encoding 적용\n",
    "train_df['대출등급'] = train_df['대출등급'].apply(lambda x: target_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, test 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(columns=['대출등급'])\n",
    "train_y = train_df['대출등급']\n",
    "\n",
    "test_x = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanLab (이상 데이터 찾기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 숫자형 데이터 StandardScaler 적용 (knn을 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"대출금액\", \"연간소득\", \"부채_대비_소득_비율\", \"연간소득/총계좌수\", \"총상환원금비율\", \"총상환이자비율\", \"총연체금액\", \"연체계좌수/총계좌수\", '연간소득/대출금액']\n",
    "scaler = StandardScaler()\n",
    "X_processed = train_x.copy()\n",
    "X_processed[numeric_features] = scaler.fit_transform(train_x[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 사용할 모델 정의 (xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "num_crossval_folds = 5\n",
    "pred_probs = cross_val_predict(\n",
    "    clf,\n",
    "    X_processed,\n",
    "    train_y,\n",
    "    cv=num_crossval_folds,\n",
    "    method=\"predict_proba\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = NearestNeighbors(metric='euclidean', n_neighbors=5) # n_neighbors=5\n",
    "KNN.fit(train_x.values)\n",
    "\n",
    "knn_graph = KNN.kneighbors_graph(mode=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상데이터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding label issues ...\n",
      "Finding outlier issues ...\n",
      "Finding near_duplicate issues ...\n",
      "Finding non_iid issues ...\n",
      "Error in non_iid: If a knn_graph is not provided, features must be provided to fit a new knn.\n",
      "Failed to check for these issue types: [NonIIDIssueManager]\n",
      "\n",
      "Audit complete. 12744 issues found in the dataset.\n",
      "Here is a summary of the different kinds of issues found in the data:\n",
      "\n",
      "    issue_type  num_issues\n",
      "       outlier       12352\n",
      "         label         198\n",
      "near_duplicate         194\n",
      "\n",
      "Dataset Information: num_examples: 96293, num_classes: 7\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 12352\n",
      "Overall dataset quality in terms of this issue: 0.0851\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_outlier_issue  outlier_score\n",
      "73607              True            0.0\n",
      "43495              True            0.0\n",
      "35472              True            0.0\n",
      "43493              True            0.0\n",
      "66125              True            0.0\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 198\n",
      "Overall dataset quality in terms of this issue: 0.9979\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_label_issue  label_score  given_label  predicted_label\n",
      "72423            True     0.000004            6                2\n",
      "18211           False     0.000018            6                1\n",
      "25241           False     0.000039            6                2\n",
      "81247           False     0.000046            6                4\n",
      "27502            True     0.000065            6                4\n",
      "\n",
      "\n",
      "------------------ near_duplicate issues -------------------\n",
      "\n",
      "About this issue:\n",
      "\tA (near) duplicate issue refers to two or more examples in\n",
      "    a dataset that are extremely similar to each other, relative\n",
      "    to the rest of the dataset.  The examples flagged with this issue\n",
      "    may be exactly duplicated, or lie atypically close together when\n",
      "    represented as vectors (i.e. feature embeddings).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 194\n",
      "Overall dataset quality in terms of this issue: 0.9282\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_near_duplicate_issue  near_duplicate_score near_duplicate_sets  distance_to_nearest_neighbor\n",
      "56198                     True              0.016150             [11934]                      0.016152\n",
      "11934                     True              0.016150             [56198]                      0.016152\n",
      "42768                     True              0.030208             [25263]                      0.030217\n",
      "25263                     True              0.030208             [42768]                      0.030217\n",
      "28352                     True              0.045965             [92221]                      0.045997\n"
     ]
    }
   ],
   "source": [
    "data = {\"X\": X_processed.values, \"y\": train_y}\n",
    "\n",
    "lab = Datalab(data, label_name=\"y\")\n",
    "lab.find_issues(pred_probs=pred_probs, knn_graph=knn_graph)\n",
    "\n",
    "# lab_df = lab.get_issues()\n",
    "# outlier_issue_index = lab_df[lab_df['is_outlier_issue']==True].index\n",
    "# len(outlier_issue_index)\n",
    "lab.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **outlier 이슈:** 12352개\n",
    "- **label 이슈:** 198개\n",
    "- **near_duplicate 이슈:** 194게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>is_outlier_issue</th>\n",
       "      <th>outlier_score</th>\n",
       "      <th>is_near_duplicate_issue</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.284084</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.997005</td>\n",
       "      <td>False</td>\n",
       "      <td>0.126714</td>\n",
       "      <td>False</td>\n",
       "      <td>0.950431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.996561</td>\n",
       "      <td>False</td>\n",
       "      <td>0.117219</td>\n",
       "      <td>False</td>\n",
       "      <td>0.895225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>False</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>False</td>\n",
       "      <td>0.844929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>False</td>\n",
       "      <td>0.128138</td>\n",
       "      <td>False</td>\n",
       "      <td>0.951158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96288</th>\n",
       "      <td>False</td>\n",
       "      <td>0.998180</td>\n",
       "      <td>False</td>\n",
       "      <td>0.049453</td>\n",
       "      <td>False</td>\n",
       "      <td>0.985153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96289</th>\n",
       "      <td>False</td>\n",
       "      <td>0.995913</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146508</td>\n",
       "      <td>False</td>\n",
       "      <td>0.841536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96290</th>\n",
       "      <td>False</td>\n",
       "      <td>0.998321</td>\n",
       "      <td>False</td>\n",
       "      <td>0.129524</td>\n",
       "      <td>False</td>\n",
       "      <td>0.938531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96291</th>\n",
       "      <td>False</td>\n",
       "      <td>0.996392</td>\n",
       "      <td>False</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96292</th>\n",
       "      <td>False</td>\n",
       "      <td>0.997783</td>\n",
       "      <td>False</td>\n",
       "      <td>0.045928</td>\n",
       "      <td>False</td>\n",
       "      <td>0.988730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96293 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_label_issue  label_score  is_outlier_issue  outlier_score  \\\n",
       "0               False     0.284084             False       0.001146   \n",
       "1               False     0.997005             False       0.126714   \n",
       "2               False     0.996561             False       0.117219   \n",
       "3               False     0.999403             False       0.100094   \n",
       "4               False     0.992861             False       0.128138   \n",
       "...               ...          ...               ...            ...   \n",
       "96288           False     0.998180             False       0.049453   \n",
       "96289           False     0.995913             False       0.146508   \n",
       "96290           False     0.998321             False       0.129524   \n",
       "96291           False     0.996392             False       0.027733   \n",
       "96292           False     0.997783             False       0.045928   \n",
       "\n",
       "       is_near_duplicate_issue  near_duplicate_score  \n",
       "0                        False              0.999920  \n",
       "1                        False              0.950431  \n",
       "2                        False              0.895225  \n",
       "3                        False              0.844929  \n",
       "4                        False              0.951158  \n",
       "...                        ...                   ...  \n",
       "96288                    False              0.985153  \n",
       "96289                    False              0.841536  \n",
       "96290                    False              0.938531  \n",
       "96291                    False              0.996791  \n",
       "96292                    False              0.988730  \n",
       "\n",
       "[96293 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_df = lab.get_issues()\n",
    "lab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상 데이터 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 이슈, duplicate 이슈 데이터 인덱스 가져오기\n",
    "label_issue_index = lab_df[lab_df['is_label_issue']==True].index\n",
    "\n",
    "duplicate_issue_index = lab_df[lab_df['is_near_duplicate_issue']==True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상 데이터 drop\n",
    "train_x.drop(label_issue_index, inplace=True)\n",
    "train_y.drop(label_issue_index, inplace=True)\n",
    "\n",
    "train_x.drop(duplicate_issue_index, inplace=True)\n",
    "train_y.drop(duplicate_issue_index, inplace=True)\n",
    "\n",
    "train_x.reset_index(drop=True, inplace=True)\n",
    "train_y.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kfold 배깅 앙상블\n",
    "\n",
    "# kfold 함수\n",
    "def model_fitting(X_train, y_train, test, model, k):\n",
    "    # k-Fold 설정\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # 각 fold의 모델로부터의 예측을 저장할 리스트와 f1 점수 리스트\n",
    "    ensemble_predictions = []\n",
    "    ensemble_predictions_train = []\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(kf.split(X_train), total=k, desc=\"Processing folds\"):\n",
    "        X_t, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_t, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # 각 모델 학습\n",
    "        model.fit(X_t, y_t)\n",
    "        \n",
    "        # 각 모델로부터 Validation set에 대한 예측을 생성\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        # Validation set에 대한 대회 평가 산식 계산 후 저장\n",
    "        score = f1_score(y_val, val_pred, average='macro')\n",
    "        scores.append(score)\n",
    "        print(score)\n",
    "        \n",
    "        #train 데이터셋에 대해 앙상블 성능평가 (train 데이터셋에 대한 예측 수행 후 저장)\n",
    "        model_pred_train = model.predict(train_x)\n",
    "        ensemble_predictions_train.append(model_pred_train)    \n",
    "        \n",
    "        # test 데이터셋에 대한 예측 수행 후 저장\n",
    "        model_pred = model.predict(test)        \n",
    "        ensemble_predictions.append(model_pred)\n",
    "        \n",
    "    # K-fold 모든 예측의 평균을 계산하여 fold별 모델들의 voting 앙상블 예측 생성\n",
    "    # test 데이터 앙상블\n",
    "    final_predictions, _ = mode(ensemble_predictions, axis=0)\n",
    "    final_predictions = final_predictions.ravel()\n",
    "    \n",
    "    # train 데이터 앙상블(성능평가용)\n",
    "    final_predictions_train, _ = mode(ensemble_predictions_train, axis=0)\n",
    "    final_predictions_train = final_predictions_train.ravel()\n",
    "    \n",
    "    # 각 fold에서의 Validation Metric Score와 전체 평균 Validation Metric Score출력\n",
    "    print(\"Validation : fl scores for each fold:\", scores)\n",
    "    print(\"Validation : fl socres mean:\", np.mean(scores))\n",
    "    print(\"emsemble : fl socres train:\", f1_score(train_y, final_predictions_train, average='macro'))\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  20%|██        | 1/5 [00:08<00:35,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9459334757584063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  40%|████      | 2/5 [00:18<00:27,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559963723573709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  60%|██████    | 3/5 [00:27<00:18,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487197944760121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  80%|████████  | 4/5 [00:37<00:09,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952537755325867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds: 100%|██████████| 5/5 [00:47<00:00,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9479777548676349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation : fl scores for each fold: [0.9459334757584063, 0.9559963723573709, 0.9487197944760121, 0.952537755325867, 0.9479777548676349]\n",
      "Validation : fl socres mean: 0.9502330305570583\n",
      "emsemble : fl socres train: 0.9920976489469265\n"
     ]
    }
   ],
   "source": [
    "# knn 5개 + label, duplicate 이슈 drop\n",
    "xgboost = xgb.XGBClassifier()\n",
    "final_prediction = model_fitting(train_x, train_y, test_x, xgboost, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission['대출등급'] = final_prediction\n",
    "sample_submission['대출등급'] = sample_submission['대출등급'].apply(lambda x: reverse_target_dict[x])\n",
    "sample_submission.to_csv('xgboost_7fold_cleanlab_label_duplicate_이슈drop.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
